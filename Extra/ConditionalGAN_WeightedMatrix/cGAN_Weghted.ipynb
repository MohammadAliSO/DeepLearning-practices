{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JMOJPaFVrrJL"
      },
      "outputs": [],
      "source": [
        "import os, time\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import pickle\n",
        "import imageio\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eETeyDLPrrJM"
      },
      "outputs": [],
      "source": [
        "# G(z)\n",
        "class generator_weight(nn.Module):\n",
        "    # initializers\n",
        "    def __init__(self):\n",
        "        super(generator_weight, self).__init__()\n",
        "        self.w_11 = nn.Parameter(torch.tensor(torch.rand(8,4), requires_grad=True))\n",
        "        self.w_12 = nn.Parameter(torch.tensor(torch.rand(4,8), requires_grad=True))\n",
        "\n",
        "        self.w_21 = nn.Parameter(torch.tensor(torch.rand(8,4), requires_grad=True))\n",
        "        self.w_22 = nn.Parameter(torch.tensor(torch.rand(4,8), requires_grad=True))\n",
        "\n",
        "        self.w_31 = nn.Parameter(torch.tensor(torch.rand(16,8), requires_grad=True))\n",
        "        self.w_32 = nn.Parameter(torch.tensor(torch.rand(8,16), requires_grad=True))\n",
        "\n",
        "        self.w_41 = nn.Parameter(torch.tensor(torch.rand(28,16), requires_grad=True))\n",
        "        self.w_42 = nn.Parameter(torch.tensor(torch.rand(16,28), requires_grad=True))\n",
        "\n",
        "    # forward method\n",
        "    def forward(self, input, label):\n",
        "        x = self.w_11 @ input @ self.w_12\n",
        "        y = self.w_21 @ label @ self.w_22\n",
        "        z = torch.cat([x.unsqueeze(1), y.unsqueeze(1)], dim=1)\n",
        "        z = torch.mean(z, dim=1)\n",
        "        z = self.w_31 @ z @ self.w_32\n",
        "        z = self.w_41 @ z @ self.w_42\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YYy4_1hWrrJM"
      },
      "outputs": [],
      "source": [
        "def normal_init(m, mean, std):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        m.weight.data.normal_(mean, std)\n",
        "        m.bias.data.zero_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LWoc91ZhrrJM"
      },
      "outputs": [],
      "source": [
        "\n",
        "class discriminator(nn.Module):\n",
        "    # initializers\n",
        "    def __init__(self):\n",
        "        super(discriminator, self).__init__()\n",
        "        self.fc1_1 = nn.Linear(784, 1024)\n",
        "        self.fc1_2 = nn.Linear(10, 1024)\n",
        "        self.fc2 = nn.Linear(2048, 512)\n",
        "        self.fc2_bn = nn.BatchNorm1d(512)\n",
        "        self.fc3 = nn.Linear(512, 256)\n",
        "        self.fc3_bn = nn.BatchNorm1d(256)\n",
        "        self.fc4 = nn.Linear(256, 1)\n",
        "\n",
        "    # weight_init\n",
        "    def weight_init(self, mean, std):\n",
        "        for m in self._modules:\n",
        "            normal_init(self._modules[m], mean, std)\n",
        "\n",
        "    # forward method\n",
        "    def forward(self, input, label):\n",
        "        x = F.leaky_relu(self.fc1_1(input), 0.2)\n",
        "        y = F.leaky_relu(self.fc1_2(label), 0.2)\n",
        "        x = torch.cat([x, y], 1)\n",
        "        x = F.leaky_relu(self.fc2_bn(self.fc2(x)), 0.2)\n",
        "        x = F.leaky_relu(self.fc3_bn(self.fc3(x)), 0.2)\n",
        "        x = F.sigmoid(self.fc4(x))\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xhykZ10zyzsb"
      },
      "outputs": [],
      "source": [
        "def lable2mat4(y):\n",
        "    matrix = torch.nn.functional.pad(y, (0, 6), value=0)\n",
        "    matrix = matrix.view(4, 4)\n",
        "    return matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RTXm0cfarrJM"
      },
      "outputs": [],
      "source": [
        "temp_z_ = torch.rand(10, 4,4)\n",
        "fixed_z_ = temp_z_\n",
        "fixed_y_ = torch.zeros(10, 1)\n",
        "for i in range(9):\n",
        "    fixed_z_ = torch.cat([fixed_z_, temp_z_], 0)\n",
        "    temp = torch.ones(10,1) + i\n",
        "    fixed_y_ = torch.cat([fixed_y_, temp], 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFmSXEwvrrJM",
        "outputId": "6617c10f-6635-4b4a-dde5-869d0783a5d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-e62a5541b6af>:1: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  fixed_z_ = Variable(fixed_z_.cuda(), volatile=True)\n",
            "<ipython-input-8-e62a5541b6af>:4: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  fixed_y_label_ = Variable(fixed_y_label_.cuda(), volatile=True)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "fixed_z_ = Variable(fixed_z_.cuda(), volatile=True)\n",
        "fixed_y_label_ = torch.zeros(100, 10)\n",
        "fixed_y_label_.scatter_(1, fixed_y_.type(torch.LongTensor), 1)\n",
        "fixed_y_label_ = Variable(fixed_y_label_.cuda(), volatile=True)\n",
        "fixed_y_label__mat_ =  torch.stack([lable2mat4(y) for y  in fixed_y_label_])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQtiEjdzrrJN",
        "outputId": "0a9861ab-798c-42f8-a364-9e1183d00371"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([100, 4, 4])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fixed_z_.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VTm5GMrprrJN"
      },
      "outputs": [],
      "source": [
        "\n",
        "def show_result(num_epoch, show = False, save = False, path = 'result.png'):\n",
        "\n",
        "    G.eval()\n",
        "    test_images = G(fixed_z_, fixed_y_label__mat_)\n",
        "    G.train()\n",
        "\n",
        "    size_figure_grid = 10\n",
        "    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(5, 5))\n",
        "    for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n",
        "        ax[i, j].get_xaxis().set_visible(False)\n",
        "        ax[i, j].get_yaxis().set_visible(False)\n",
        "\n",
        "    for k in range(10*10):\n",
        "        i = k // 10\n",
        "        j = k % 10\n",
        "        ax[i, j].cla()\n",
        "        ax[i, j].imshow(test_images[k].cpu().data.view(28, 28).numpy(), cmap='gray')\n",
        "\n",
        "    label = 'Epoch {0}'.format(num_epoch)\n",
        "    fig.text(0.5, 0.04, label, ha='center')\n",
        "    plt.savefig(path)\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VYtcla4NrrJN"
      },
      "outputs": [],
      "source": [
        "\n",
        "def show_train_hist(hist, show = False, save = False, path = 'Train_hist.png'):\n",
        "    x = range(len(hist['D_losses']))\n",
        "\n",
        "    y1 = hist['D_losses']\n",
        "    y2 = hist['G_losses']\n",
        "\n",
        "    plt.plot(x, y1, label='D_loss')\n",
        "    plt.plot(x, y2, label='G_loss')\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    plt.legend(loc=4)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save:\n",
        "        plt.savefig(path)\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Di9y_IdWrrJN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# training parameters\n",
        "batch_size = 128\n",
        "lr = 0.0002\n",
        "train_epoch = 50\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "AM2sUfw7rrJN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# data_loader\n",
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.5, ), std=(0.5, ))\n",
        "])\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=True, download=True, transform=transform),\n",
        "    batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCuAC68UrrJN",
        "outputId": "fc69c305-de6c-4d39-830d-b1840935a2fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-b4659ca00ff3>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.w_11 = nn.Parameter(torch.tensor(torch.rand(8,4), requires_grad=True))\n",
            "<ipython-input-2-b4659ca00ff3>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.w_12 = nn.Parameter(torch.tensor(torch.rand(4,8), requires_grad=True))\n",
            "<ipython-input-2-b4659ca00ff3>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.w_21 = nn.Parameter(torch.tensor(torch.rand(8,4), requires_grad=True))\n",
            "<ipython-input-2-b4659ca00ff3>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.w_22 = nn.Parameter(torch.tensor(torch.rand(4,8), requires_grad=True))\n",
            "<ipython-input-2-b4659ca00ff3>:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.w_31 = nn.Parameter(torch.tensor(torch.rand(16,8), requires_grad=True))\n",
            "<ipython-input-2-b4659ca00ff3>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.w_32 = nn.Parameter(torch.tensor(torch.rand(8,16), requires_grad=True))\n",
            "<ipython-input-2-b4659ca00ff3>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.w_41 = nn.Parameter(torch.tensor(torch.rand(28,16), requires_grad=True))\n",
            "<ipython-input-2-b4659ca00ff3>:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.w_42 = nn.Parameter(torch.tensor(torch.rand(16,28), requires_grad=True))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "discriminator(\n",
              "  (fc1_1): Linear(in_features=784, out_features=1024, bias=True)\n",
              "  (fc1_2): Linear(in_features=10, out_features=1024, bias=True)\n",
              "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "  (fc2_bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc3): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (fc3_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc4): Linear(in_features=256, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# network\n",
        "G = generator_rezhghi()\n",
        "D = discriminator()\n",
        "D.weight_init(mean=0, std=0.02)\n",
        "G.cuda()\n",
        "D.cuda()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9XDlq7WFrrJN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Binary Cross Entropy loss\n",
        "BCE_loss = nn.BCELoss()\n",
        "\n",
        "# Adam optimizer\n",
        "G_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "D_optimizer = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "U5ZyJlVZrrJN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# results save folder\n",
        "if not os.path.isdir('MNIST_cGAN_results'):\n",
        "    os.mkdir('MNIST_cGAN_results')\n",
        "if not os.path.isdir('MNIST_cGAN_results/Fixed_results'):\n",
        "    os.mkdir('MNIST_cGAN_results/Fixed_results')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wdDK7KjVrrJO"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_hist = {}\n",
        "train_hist['D_losses'] = []\n",
        "train_hist['G_losses'] = []\n",
        "train_hist['per_epoch_ptimes'] = []\n",
        "train_hist['total_ptime'] = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJR-87Ye_HaR",
        "outputId": "3ed9c80c-01d1-4b75-d147-b4616b45cb2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.2.1+cu121)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "F7mH4GQ98kAk"
      },
      "outputs": [],
      "source": [
        "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
        "ssim = StructuralSimilarityIndexMeasure(data_range=1.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmcdJcg2rrJO",
        "outputId": "482aae9d-2d3c-48df-c811-6ed327bc2ff7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training start!\n",
            "[1/50] - ptime: 41.00, loss_d: 0.710, loss_g: 1.341, ssim_score:-0.000000\n",
            "[2/50] - ptime: 33.18, loss_d: 0.180, loss_g: 2.562, ssim_score:-0.000000\n",
            "[3/50] - ptime: 34.41, loss_d: 0.073, loss_g: 3.505, ssim_score:-0.000000\n",
            "[4/50] - ptime: 33.21, loss_d: 0.052, loss_g: 3.980, ssim_score:-0.000000\n",
            "[5/50] - ptime: 33.34, loss_d: 0.021, loss_g: 4.742, ssim_score:-0.000000\n",
            "[6/50] - ptime: 32.94, loss_d: 0.033, loss_g: 4.547, ssim_score:-0.000000\n",
            "[7/50] - ptime: 33.38, loss_d: 0.011, loss_g: 5.292, ssim_score:-0.000000\n",
            "[8/50] - ptime: 34.32, loss_d: 0.019, loss_g: 5.693, ssim_score:-0.000003\n",
            "[9/50] - ptime: 33.44, loss_d: 0.014, loss_g: 5.323, ssim_score:-0.000039\n",
            "[10/50] - ptime: 33.59, loss_d: 0.015, loss_g: 5.408, ssim_score:0.000033\n",
            "[11/50] - ptime: 33.94, loss_d: 0.014, loss_g: 5.784, ssim_score:0.000011\n",
            "[12/50] - ptime: 34.18, loss_d: 0.005, loss_g: 6.181, ssim_score:-0.000001\n",
            "[13/50] - ptime: 33.82, loss_d: 0.003, loss_g: 7.062, ssim_score:-0.000003\n",
            "[14/50] - ptime: 33.85, loss_d: 0.002, loss_g: 6.956, ssim_score:-0.000002\n",
            "[15/50] - ptime: 34.00, loss_d: 0.013, loss_g: 6.920, ssim_score:-0.000008\n",
            "[16/50] - ptime: 34.20, loss_d: 0.005, loss_g: 6.144, ssim_score:-0.000005\n",
            "[17/50] - ptime: 33.83, loss_d: 0.002, loss_g: 6.915, ssim_score:-0.000005\n",
            "[18/50] - ptime: 33.95, loss_d: 0.002, loss_g: 7.302, ssim_score:-0.000013\n",
            "[19/50] - ptime: 34.43, loss_d: 0.010, loss_g: 6.428, ssim_score:-0.000034\n",
            "[20/50] - ptime: 34.21, loss_d: 0.002, loss_g: 6.922, ssim_score:-0.000046\n",
            "[21/50] - ptime: 35.18, loss_d: 0.002, loss_g: 7.302, ssim_score:0.000031\n",
            "[22/50] - ptime: 33.40, loss_d: 0.015, loss_g: 6.231, ssim_score:0.000188\n",
            "[23/50] - ptime: 33.96, loss_d: 0.013, loss_g: 7.045, ssim_score:-0.000058\n",
            "[24/50] - ptime: 34.07, loss_d: 0.003, loss_g: 7.228, ssim_score:-0.000555\n",
            "[25/50] - ptime: 33.50, loss_d: 0.003, loss_g: 7.024, ssim_score:0.001447\n",
            "[26/50] - ptime: 33.47, loss_d: 0.002, loss_g: 7.341, ssim_score:0.002346\n",
            "[27/50] - ptime: 34.26, loss_d: 0.006, loss_g: 7.185, ssim_score:0.001588\n",
            "[28/50] - ptime: 34.07, loss_d: 0.003, loss_g: 6.913, ssim_score:0.000216\n",
            "[29/50] - ptime: 33.82, loss_d: 0.009, loss_g: 6.592, ssim_score:-0.000898\n",
            "learning rate change!\n",
            "[30/50] - ptime: 33.54, loss_d: 0.002, loss_g: 6.944, ssim_score:0.000196\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print('training start!')\n",
        "start_time = time.time()\n",
        "for epoch in range(train_epoch):\n",
        "    D_losses = []\n",
        "    G_losses = []\n",
        "    ssim_scores = []\n",
        "\n",
        "    # learning rate decay\n",
        "    if (epoch+1) == 30:\n",
        "        G_optimizer.param_groups[0]['lr'] /= 10\n",
        "        D_optimizer.param_groups[0]['lr'] /= 10\n",
        "        print(\"learning rate change!\")\n",
        "\n",
        "    if (epoch+1) == 40:\n",
        "        G_optimizer.param_groups[0]['lr'] /= 10\n",
        "        D_optimizer.param_groups[0]['lr'] /= 10\n",
        "        print(\"learning rate change!\")\n",
        "\n",
        "    epoch_start_time = time.time()\n",
        "    for x_, y_ in train_loader:\n",
        "        # train discriminator D\n",
        "        D.zero_grad()\n",
        "        mini_batch = x_.size()[0]\n",
        "        if(mini_batch < 128):\n",
        "            continue\n",
        "        y_real_ = torch.ones(mini_batch)\n",
        "        y_fake_ = torch.zeros(mini_batch)\n",
        "        y_label_ = torch.zeros(mini_batch, 10)\n",
        "        y_label_.scatter_(1, y_.view(mini_batch, 1), 1)\n",
        "\n",
        "        x_mat_ = x_\n",
        "        x_ = x_.view(-1, 28 * 28)\n",
        "        x_, y_label_, y_real_, y_fake_ = Variable(x_.cuda()), Variable(y_label_.cuda()), Variable(y_real_.cuda()), Variable(y_fake_.cuda())\n",
        "        D_result = D(x_, y_label_).squeeze()\n",
        "        D_real_loss = BCE_loss(D_result, y_real_)\n",
        "\n",
        "        z_ = torch.rand((128, 4,4))\n",
        "        y_ = (torch.rand(mini_batch, 1) * 10).type(torch.LongTensor)\n",
        "        y_label_ = torch.zeros(mini_batch, 10)\n",
        "        y_label_.scatter_(1, y_.view(mini_batch, 1), 1)\n",
        "\n",
        "        z_, y_label_ = Variable(z_.cuda()), Variable(y_label_.cuda())\n",
        "        y_lable_mat_ =  torch.stack([lable2mat4(y) for y  in y_label_])\n",
        "\n",
        "        G_result = G(z_, y_lable_mat_)\n",
        "        G_result = G_result.view(-1, 28 * 28)\n",
        "        D_result = D(G_result, y_label_).squeeze()\n",
        "        D_fake_loss = BCE_loss(D_result, y_fake_)\n",
        "        D_fake_score = D_result.data.mean()\n",
        "\n",
        "        D_train_loss = D_real_loss + D_fake_loss\n",
        "\n",
        "        D_train_loss.backward()\n",
        "        D_optimizer.step()\n",
        "\n",
        "        D_losses.append(D_train_loss.data)\n",
        "\n",
        "        # train generator G\n",
        "        G.zero_grad()\n",
        "\n",
        "        z_ = torch.rand((128, 4,4))\n",
        "        y_ = (torch.rand(mini_batch, 1) * 10).type(torch.LongTensor)\n",
        "        y_label_ = torch.zeros(mini_batch, 10)\n",
        "        y_label_.scatter_(1, y_.view(mini_batch, 1), 1)\n",
        "        z_, y_label_ = Variable(z_.cuda()), Variable(y_label_.cuda())\n",
        "        y_lable_mat_ =  torch.stack([lable2mat4(y) for y  in y_label_])\n",
        "\n",
        "        G_result_mat_ = G(z_, y_lable_mat_)\n",
        "        G_result = G_result_mat_.view(-1, 28 * 28)\n",
        "        D_result = D(G_result, y_label_).squeeze()\n",
        "        G_train_loss = BCE_loss(D_result, y_real_)\n",
        "        G_train_loss.backward()\n",
        "        G_optimizer.step()\n",
        "\n",
        "        G_losses.append(G_train_loss.data)\n",
        "\n",
        "        ssim_score = (float)(ssim( G_result_mat_.unsqueeze(1).cpu() , x_mat_.cpu() ))\n",
        "        ssim_scores.append(ssim_score)\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    per_epoch_ptime = epoch_end_time - epoch_start_time\n",
        "\n",
        "\n",
        "    print('[%d/%d] - ptime: %.2f, loss_d: %.3f, loss_g: %.3f, ssim_score:%f' % ((epoch + 1), train_epoch, per_epoch_ptime, torch.mean(torch.FloatTensor(D_losses)),\n",
        "                                                              torch.mean(torch.FloatTensor(G_losses)),torch.mean(torch.FloatTensor(ssim_scores))))\n",
        "    fixed_p = 'MNIST_cGAN_results/Fixed_results/MNIST_cGAN_' + str(epoch + 1) + '.png'\n",
        "    show_result((epoch+1), save=True, path=fixed_p)\n",
        "    train_hist['D_losses'].append(torch.mean(torch.FloatTensor(D_losses)))\n",
        "    train_hist['G_losses'].append(torch.mean(torch.FloatTensor(G_losses)))\n",
        "    train_hist['per_epoch_ptimes'].append(per_epoch_ptime)\n",
        "\n",
        "end_time = time.time()\n",
        "total_ptime = end_time - start_time\n",
        "train_hist['total_ptime'].append(total_ptime)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqOzwPZ8rrJO"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"Avg one epoch ptime: %.2f, total %d epochs ptime: %.2f\" % (torch.mean(torch.FloatTensor(train_hist['per_epoch_ptimes'])), train_epoch, total_ptime))\n",
        "print(\"Training finish!... save training results\")\n",
        "torch.save(G.state_dict(), \"MNIST_cGAN_results/generator_param.pkl\")\n",
        "torch.save(D.state_dict(), \"MNIST_cGAN_results/discriminator_param.pkl\")\n",
        "with open('MNIST_cGAN_results/train_hist.pkl', 'wb') as f:\n",
        "    pickle.dump(train_hist, f)\n",
        "\n",
        "show_train_hist(train_hist, save=True, path='MNIST_cGAN_results/MNIST_cGAN_train_hist.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NTlPoDXrrJO"
      },
      "outputs": [],
      "source": [
        "\n",
        "images = []\n",
        "for e in range(train_epoch):\n",
        "    img_name = 'MNIST_cGAN_results/Fixed_results/MNIST_cGAN_' + str(e + 1) + '.png'\n",
        "    images.append(imageio.imread(img_name))\n",
        "imageio.mimsave('MNIST_cGAN_results/generation_animation.gif', images, fps=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvIlMmlFwipu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
